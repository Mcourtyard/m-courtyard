#!/usr/bin/env python3
"""Generate dataset using Ollama local Chat API.

Key design:
 - Uses /api/chat with think:false to disable thinking mode (GLM/Qwen3 etc.)
 - num_predict=2048 to ensure enough tokens for JSON output
 - Reads both 'content' and 'thinking' fields from response
 - Incremental save: each success is appended to file immediately
 - Resume: on restart, skips already-processed segments
 - Emits detailed log events for real-time frontend display
"""

import argparse
import json
import os
import re
import sys
import urllib.request
import urllib.error


def emit(event_type, **kwargs):
    payload = {"type": event_type, **kwargs}
    print(json.dumps(payload, ensure_ascii=False), flush=True)


# â”€â”€ System prompts per mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SYSTEM_PROMPTS = {
    "qa": (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è®­ç»ƒæ•°æ®ç”Ÿæˆä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç»™å®šæ–‡æœ¬ï¼Œç”Ÿæˆä¸€ä¸ªé«˜è´¨é‡çš„é—®ç­”å¯¹ã€‚\n"
        "è¦æ±‚ï¼š\n"
        "1. é—®é¢˜åº”è¯¥æœ‰æ·±åº¦ï¼Œä¸è¦ç®€å•çš„äº‹å®æå–ï¼Œè¦è€ƒéªŒç†è§£åŠ›å’Œåˆ†æåŠ›\n"
        "2. é—®é¢˜ç±»å‹è¦å¤šæ ·ï¼šå¯ä»¥æ˜¯ç†è§£å‹ã€åˆ†æå‹ã€æ¨ç†å‹ã€åº”ç”¨å‹\n"
        "3. ç­”æ¡ˆè¦å®Œæ•´ã€æœ‰æ¡ç†ï¼ŒåŒ…å«è¶³å¤Ÿçš„ç»†èŠ‚å’Œè§£é‡Š\n"
        "4. ç­”æ¡ˆåº”åŸºäºæ–‡æœ¬å†…å®¹ä½†ç”¨è‡ªå·±çš„è¯­è¨€ç»„ç»‡ï¼Œä¸è¦ç›´æ¥å¤åˆ¶åŸæ–‡\n"
        "5. ç›´æ¥è¾“å‡ºJSONï¼Œæ ¼å¼ï¼š{\"question\": \"...\", \"answer\": \"...\"}"
    ),
    "style": (
        "ä½ æ˜¯ä¸€ä¸ªå†™ä½œé£æ ¼åˆ†æä¸æ¨¡ä»¿ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š\n"
        "1. æ·±å…¥åˆ†æç»™å®šå†™ä½œæ ·æœ¬çš„é£æ ¼ç‰¹å¾ï¼ˆåŒ…æ‹¬ï¼šç”¨è¯ä¹ æƒ¯ã€å¥å¼ç»“æ„ã€ä¿®è¾æ‰‹æ³•ã€å™äº‹è§†è§’ã€æƒ…æ„ŸåŸºè°ƒã€èŠ‚å¥éŸµå¾‹ç­‰ï¼‰\n"
        "2. åŸºäºåˆ†æå‡ºçš„é£æ ¼ï¼Œåˆ›å»ºä¸€æ¡\"å†™ä½œæŒ‡ä»¤\"å’Œ\"é£æ ¼åŒ–å›å¤\"ï¼š\n"
        "   - instructionï¼ˆå†™ä½œæŒ‡ä»¤ï¼‰ï¼šä¸€ä¸ªåˆ›æ„å†™ä½œæç¤ºï¼Œè¦æ±‚æ’°å†™ä¸€æ®µå…¨æ–°å†…å®¹ï¼ˆæ–°åœºæ™¯ã€æ–°äººç‰©ã€æ–°æƒ…èŠ‚ï¼‰ï¼Œä½†è¦æ±‚ä¿æŒä¸åŸæ–‡ä¸€è‡´çš„å†™ä½œé£æ ¼\n"
        "   - outputï¼ˆé£æ ¼åŒ–å›å¤ï¼‰ï¼šæ ¹æ®æŒ‡ä»¤åˆ›ä½œçš„å…¨æ–°æ–‡æœ¬ï¼Œå®Œç¾ä½“ç°åŸæ–‡çš„å†™ä½œé£æ ¼ç‰¹å¾\n\n"
        "æå…¶é‡è¦çš„è§„åˆ™ï¼š\n"
        "- output å¿…é¡»æ˜¯ä½ å…¨æ–°åˆ›ä½œçš„å†…å®¹ï¼Œç»å¯¹ä¸èƒ½å¤åˆ¶ã€æ”¹å†™æˆ–æ€»ç»“åŸæ–‡\n"
        "- output çš„åœºæ™¯ã€äººç‰©ã€æƒ…èŠ‚å¿…é¡»ä¸åŸæ–‡å®Œå…¨ä¸åŒ\n"
        "- output çš„å†™ä½œé£æ ¼ï¼ˆç”¨è¯ã€å¥å¼ã€ä¿®è¾ã€è¯­æ°”ï¼‰å¿…é¡»ä¸åŸæ–‡é«˜åº¦ä¸€è‡´\n"
        "- instruction ä¸è¦åŒ…å«åŸæ–‡å†…å®¹ï¼Œåªæè¿°å†™ä½œä»»åŠ¡\n"
        "ç›´æ¥è¾“å‡ºJSONï¼Œæ ¼å¼ï¼š{\"instruction\": \"...\", \"output\": \"...\"}"
    ),
    "chat": (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¯¹è¯æ•°æ®ç”Ÿæˆä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ç»™å®šæ–‡æœ¬è½¬æ¢ä¸ºè‡ªç„¶ã€æœ‰æ·±åº¦çš„å¤šè½®å¯¹è¯ï¼ˆè‡³å°‘3è½®ï¼‰ã€‚\n"
        "è¦æ±‚ï¼š\n"
        "1. å¯¹è¯åº”è¯¥è‡ªç„¶æµç•…ï¼ŒåƒçœŸå®çš„å¸ˆç”Ÿé—®ç­”æˆ–æœ‹å‹è®¨è®º\n"
        "2. ç”¨æˆ·çš„é—®é¢˜åº”å±‚å±‚é€’è¿›ï¼Œä»åŸºç¡€é—®é¢˜åˆ°æ·±å…¥æ¢è®¨\n"
        "3. åŠ©æ‰‹çš„å›ç­”åº”ä¸“ä¸šã€è¯¦ç»†ï¼Œå¼•å¯¼å¯¹è¯æ·±å…¥\n"
        "4. åŒ…å«è¿½é—®ã€æ¾„æ¸…ã€ä¸¾ä¾‹ç­‰è‡ªç„¶å¯¹è¯å…ƒç´ \n"
        "5. ä¸è¦ç®€å•åœ°æŠŠæ–‡æœ¬æ‹†åˆ†æˆå¯¹è¯ï¼Œè€Œæ˜¯å›´ç»•æ–‡æœ¬ä¸»é¢˜å±•å¼€è®¨è®º\n"
        "ç›´æ¥è¾“å‡ºJSONï¼Œæ ¼å¼ï¼š{\"conversations\": [{\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}"
    ),
    "instruct": (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŒ‡ä»¤æ•°æ®ç”Ÿæˆä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç»™å®šæ–‡æœ¬ç”Ÿæˆä¸€ä¸ªé«˜è´¨é‡çš„æŒ‡ä»¤-è¾“å‡ºå¯¹ã€‚\n"
        "è¦æ±‚ï¼š\n"
        "1. æŒ‡ä»¤ç±»å‹è¦å¤šæ ·åŒ–ï¼Œå¯ä»¥æ˜¯ï¼šæ€»ç»“ã€åˆ†æã€æ¯”è¾ƒã€æ¨ç†ã€è§£é‡Šã€æ”¹å†™ã€æ‰©å±•ã€è¯„ä»·ç­‰\n"
        "2. æŒ‡ä»¤åº”è¯¥æ˜ç¡®ã€å…·ä½“ï¼Œè®©æ¨¡å‹çŸ¥é“éœ€è¦åšä»€ä¹ˆ\n"
        "3. è¾“å‡ºåº”è¯¥é«˜è´¨é‡ã€æœ‰æ¡ç†ï¼Œå±•ç¤ºè‰¯å¥½çš„ç†è§£å’Œè¡¨è¾¾èƒ½åŠ›\n"
        "4. è¾“å‡ºä¸è¦ç›´æ¥å¤åˆ¶åŸæ–‡ï¼Œè€Œæ˜¯åŸºäºç†è§£åç”¨è‡ªå·±çš„è¯­è¨€é‡æ–°ç»„ç»‡\n"
        "ç›´æ¥è¾“å‡ºJSONï¼Œæ ¼å¼ï¼š{\"instruction\": \"...\", \"output\": \"...\"}"
    ),
}

USER_TEMPLATES = {
    "qa": (
        "è¯·æ ¹æ®ä»¥ä¸‹æ–‡æœ¬ç”Ÿæˆä¸€ä¸ªæœ‰æ·±åº¦çš„é—®ç­”å¯¹ã€‚é—®é¢˜åº”è€ƒéªŒç†è§£å’Œåˆ†æèƒ½åŠ›ï¼Œ"
        "ç­”æ¡ˆè¦å®Œæ•´æœ‰æ¡ç†ã€‚åªè¾“å‡ºJSONã€‚\n\n"
        "ã€æ–‡æœ¬å†…å®¹ã€‘\n{text}"
    ),
    "style": (
        "è¯·ä»”ç»†åˆ†æä»¥ä¸‹å†™ä½œæ ·æœ¬çš„é£æ ¼ç‰¹å¾ï¼ˆç”¨è¯ã€å¥å¼ã€ä¿®è¾ã€è¯­æ°”ã€èŠ‚å¥ç­‰ï¼‰ï¼Œ"
        "ç„¶ååˆ›å»ºä¸€æ¡å…¨æ–°çš„å†™ä½œæŒ‡ä»¤å’Œå¯¹åº”çš„é£æ ¼åŒ–å›å¤ã€‚\n"
        "æ³¨æ„ï¼šoutputå¿…é¡»æ˜¯å…¨æ–°åˆ›ä½œï¼Œåœºæ™¯å’Œå†…å®¹ä¸åŸæ–‡å®Œå…¨ä¸åŒï¼Œä½†å†™ä½œé£æ ¼é«˜åº¦ä¸€è‡´ã€‚"
        "åªè¾“å‡ºJSONã€‚\n\n"
        "ã€å†™ä½œæ ·æœ¬ã€‘\n{text}"
    ),
    "chat": (
        "è¯·å°†ä»¥ä¸‹æ–‡æœ¬çš„å†…å®¹è½¬åŒ–ä¸ºä¸€æ®µè‡ªç„¶çš„å¤šè½®å¯¹è¯ï¼ˆè‡³å°‘3è½®å¾€è¿”ï¼‰ã€‚"
        "å¯¹è¯åº”å±‚å±‚é€’è¿›ï¼ŒåŒ…å«è¿½é—®å’Œæ·±å…¥æ¢è®¨ã€‚åªè¾“å‡ºJSONã€‚\n\n"
        "ã€æ–‡æœ¬å†…å®¹ã€‘\n{text}"
    ),
    "instruct": (
        "è¯·æ ¹æ®ä»¥ä¸‹æ–‡æœ¬ç”Ÿæˆä¸€ä¸ªé«˜è´¨é‡çš„æŒ‡ä»¤-è¾“å‡ºå¯¹ã€‚"
        "æŒ‡ä»¤ç±»å‹è¯·ä»ä»¥ä¸‹ä¸­é€‰æ‹©ï¼šæ€»ç»“è¦ç‚¹ã€æ·±å…¥åˆ†æã€å¯¹æ¯”è¯´æ˜ã€å› æœæ¨ç†ã€æ¦‚å¿µè§£é‡Šã€è§‚ç‚¹è¯„ä»·ã€‚"
        "åªè¾“å‡ºJSONã€‚\n\n"
        "ã€æ–‡æœ¬å†…å®¹ã€‘\n{text}"
    ),
}


def call_ollama(model: str, system_prompt: str, user_message: str,
                temperature: float = 0.7, num_predict: int = 2048) -> dict:
    """Call Ollama Chat API. Returns the full API response dict for inspection."""
    url = "http://localhost:11434/api/chat"
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
        "stream": False,
        "think": False,
        "options": {
            "num_predict": num_predict,
            "temperature": temperature,
        }
    }
    data = json.dumps(payload).encode("utf-8")
    req = urllib.request.Request(
        url, data=data,
        headers={"Content-Type": "application/json"},
    )
    with urllib.request.urlopen(req, timeout=300) as resp:
        return json.loads(resp.read().decode("utf-8"))


def text_similarity(a: str, b: str) -> float:
    """Simple character-level Jaccard similarity between two texts."""
    if not a or not b:
        return 0.0
    # Use character n-grams (bigrams) for comparison
    def bigrams(text):
        text = text.replace(" ", "").replace("\n", "")
        return set(text[i:i+2] for i in range(len(text) - 1)) if len(text) > 1 else {text}
    set_a = bigrams(a)
    set_b = bigrams(b)
    if not set_a or not set_b:
        return 0.0
    intersection = len(set_a & set_b)
    union = len(set_a | set_b)
    return intersection / union if union > 0 else 0.0


def extract_text_from_response(api_result: dict) -> str:
    """Extract usable text from Ollama response, checking both content and thinking fields."""
    msg = api_result.get("message", {})
    content = msg.get("content", "") or ""
    thinking = msg.get("thinking", "") or ""

    # Prefer content if non-empty
    if content.strip():
        return content.strip()

    # Fallback: try to find JSON inside thinking field
    if thinking.strip():
        return thinking.strip()

    return ""


def repair_json_string(s: str) -> str:
    """Try to fix common JSON issues from LLM output.

    Handles: unescaped quotes within string values, trailing commas,
    unescaped newlines, etc.
    """
    # Replace Chinese quotes with standard quotes
    s = s.replace('\u201c', '"').replace('\u201d', '"')
    s = s.replace('\u2018', "'").replace('\u2019', "'")
    # Fix unescaped newlines within JSON strings
    # (newlines that are not preceded by a backslash)
    # We do this by replacing literal newlines inside string values
    result = []
    in_string = False
    escape_next = False
    for ch in s:
        if escape_next:
            result.append(ch)
            escape_next = False
            continue
        if ch == '\\':
            result.append(ch)
            escape_next = True
            continue
        if ch == '"':
            in_string = not in_string
            result.append(ch)
            continue
        if in_string and ch == '\n':
            result.append('\\n')
            continue
        if in_string and ch == '\t':
            result.append('\\t')
            continue
        result.append(ch)
    return ''.join(result)


def extract_key_value_fallback(text: str, mode: str) -> dict | None:
    """Last-resort extraction: find key fields by regex patterns."""
    if mode in ("style", "instruct"):
        inst_m = re.search(r'"instruction"\s*:\s*"((?:[^"\\]|\\.)*)"', text, re.DOTALL)
        out_m = re.search(r'"output"\s*:\s*"((?:[^"\\]|\\.)*)"', text, re.DOTALL)
        if not inst_m or not out_m:
            # Try with greedy match for unescaped quotes in values
            inst_m = re.search(r'"instruction"\s*:\s*"(.+?)"\s*,\s*"output"', text, re.DOTALL)
            out_m = re.search(r'"output"\s*:\s*"(.+?)"\s*}', text, re.DOTALL)
        if inst_m and out_m:
            return {"instruction": inst_m.group(1), "output": out_m.group(1)}
    elif mode == "qa":
        q_m = re.search(r'"question"\s*:\s*"((?:[^"\\]|\\.)*)"', text, re.DOTALL)
        a_m = re.search(r'"answer"\s*:\s*"((?:[^"\\]|\\.)*)"', text, re.DOTALL)
        if not q_m or not a_m:
            q_m = re.search(r'"question"\s*:\s*"(.+?)"\s*,\s*"answer"', text, re.DOTALL)
            a_m = re.search(r'"answer"\s*:\s*"(.+?)"\s*}', text, re.DOTALL)
        if q_m and a_m:
            return {"question": q_m.group(1), "answer": a_m.group(1)}
    return None


def parse_json_response(text: str, mode: str = "") -> dict | None:
    """Robustly extract JSON object from model response."""
    if not text:
        return None

    cleaned = text.strip()

    # 1. Strip markdown code blocks: ```json ... ``` or ``` ... ```
    code_block = re.search(r'```(?:json)?\s*\n?(.*?)\n?\s*```', cleaned, re.DOTALL)
    if code_block:
        cleaned = code_block.group(1).strip()

    # 2. Try direct parse
    try:
        obj = json.loads(cleaned)
        if isinstance(obj, dict):
            return obj
    except json.JSONDecodeError:
        pass

    # 3. Try with JSON repair (fix unescaped quotes/newlines)
    try:
        repaired = repair_json_string(cleaned)
        obj = json.loads(repaired)
        if isinstance(obj, dict):
            return obj
    except json.JSONDecodeError:
        pass

    # 4. Find outermost balanced { ... } and try parsing
    depth = 0
    start = -1
    for i, ch in enumerate(cleaned):
        if ch == '{':
            if depth == 0:
                start = i
            depth += 1
        elif ch == '}':
            depth -= 1
            if depth == 0 and start >= 0:
                candidate = cleaned[start:i + 1]
                try:
                    obj = json.loads(candidate)
                    if isinstance(obj, dict):
                        return obj
                except json.JSONDecodeError:
                    # Try repair on the candidate
                    try:
                        repaired = repair_json_string(candidate)
                        obj = json.loads(repaired)
                        if isinstance(obj, dict):
                            return obj
                    except json.JSONDecodeError:
                        pass
                start = -1

    # 5. Regex-based key-value extraction as last resort
    if mode:
        result = extract_key_value_fallback(text, mode)
        if result:
            return result

    # 6. Find any JSON-like pattern
    for m in re.finditer(r'\{[^{}]*\}', text):
        try:
            obj = json.loads(m.group())
            if isinstance(obj, dict):
                return obj
        except json.JSONDecodeError:
            continue

    return None


def to_chat_format(data: dict, mode: str) -> dict | None:
    """Convert to unified chat messages format."""
    if mode == "qa":
        q = data.get("question", "")
        a = data.get("answer", "")
        if q and a:
            return {"messages": [
                {"role": "user", "content": str(q)},
                {"role": "assistant", "content": str(a)},
            ]}
    elif mode in ("style", "instruct"):
        inst = data.get("instruction", "")
        out = data.get("output", "")
        if inst and out:
            return {"messages": [
                {"role": "user", "content": str(inst)},
                {"role": "assistant", "content": str(out)},
            ]}
    elif mode == "chat":
        convs = data.get("conversations", [])
        if convs and len(convs) >= 2:
            return {"messages": convs}
    return None


def load_existing_progress(dataset_dir: str) -> int:
    """Count existing lines in train.jsonl to support resume."""
    train_path = os.path.join(dataset_dir, "train.jsonl")
    if not os.path.exists(train_path):
        return 0
    count = 0
    with open(train_path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                count += 1
    return count


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--project-dir", required=True)
    parser.add_argument("--output-dir", default=None, help="Output directory for dataset files")
    parser.add_argument("--model", required=True)
    parser.add_argument("--mode", default="qa", choices=["qa", "style", "chat", "instruct"])
    parser.add_argument("--resume", action="store_true", help="Resume from previous progress")
    args = parser.parse_args()

    segments_path = os.path.join(args.project_dir, "cleaned", "segments.jsonl")
    if not os.path.exists(segments_path):
        emit("error", message="æœªæ‰¾åˆ° segments.jsonlï¼Œè¯·å…ˆæ‰§è¡Œæ¸…æ´—ã€‚")
        sys.exit(1)

    # Load all segments
    segments = []
    with open(segments_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                try:
                    obj = json.loads(line)
                    text = obj.get("text", "")
                    if text and len(text) >= 20:
                        segments.append(text)
                except json.JSONDecodeError:
                    continue

    if not segments:
        emit("error", message="æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ–‡æœ¬æ®µè½ã€‚")
        sys.exit(1)

    dataset_dir = args.output_dir if args.output_dir else os.path.join(args.project_dir, "dataset")
    os.makedirs(dataset_dir, exist_ok=True)
    train_path = os.path.join(dataset_dir, "train.jsonl")
    valid_path = os.path.join(dataset_dir, "valid.jsonl")

    # Check for resume
    skip_count = 0
    if args.resume:
        skip_count = load_existing_progress(dataset_dir)
        if skip_count > 0:
            emit("log", message=f"ğŸ”„ æ£€æµ‹åˆ°å·²æœ‰ {skip_count} æ¡æ•°æ®ï¼Œä»ç¬¬ {skip_count + 1} æ®µç»§ç»­...")

    total = len(segments)
    emit("progress", step=skip_count, total=total,
         desc=f"ä½¿ç”¨ [{args.model}] ç”Ÿæˆæ•°æ®é›†...")
    emit("log", message=f"ğŸ“¡ è¿æ¥ Ollama...\n   æ¨¡å‹: {args.model}\n   æ¨¡å¼: {args.mode}\n   æ–‡æœ¬æ®µæ•°: {total}\n   è·³è¿‡å·²å®Œæˆ: {skip_count}")

    # Verify connection with a simple test
    try:
        test_result = call_ollama(args.model, "ä½ å¥½", "å›å¤OK", )
        test_content = extract_text_from_response(test_result)
        done_reason = test_result.get("done_reason", "unknown")
        emit("log", message=f"âœ… Ollama è¿æ¥æˆåŠŸ\n   æ¨¡å‹å“åº”: {test_content[:80]}\n   å®ŒæˆåŸå› : {done_reason}")
    except Exception as e:
        emit("log", message=f"âŒ Ollama è¿æ¥å¤±è´¥: {e}")
        emit("error", message=f"æ— æ³•è¿æ¥ Ollama: {e}")
        sys.exit(1)

    system_prompt = SYSTEM_PROMPTS[args.mode]
    user_template = USER_TEMPLATES[args.mode]
    # Use higher temperature for style mode to encourage creativity
    temp = 0.9 if args.mode == "style" else 0.7
    success_count = skip_count
    failed = 0
    similarity_rejected = 0

    # Open files for incremental append
    file_mode = "a" if args.resume and skip_count > 0 else "w"
    train_file = open(train_path, file_mode, encoding="utf-8")

    try:
        for i in range(skip_count, total):
            text = segments[i]
            segment_preview = text[:80].replace("\n", " ")
            emit("log", message=f"\nâ”€â”€ ç¬¬ {i+1}/{total} æ®µ â”€â”€\nğŸ“„ æ–‡æœ¬: {segment_preview}...")

            try:
                user_msg = user_template.format(text=text[:2000])
                # Style mode needs more tokens for creative content
                n_predict = 4096 if args.mode == "style" else 2048
                api_result = call_ollama(args.model, system_prompt, user_msg, temperature=temp, num_predict=n_predict)

                # Extract text from response (handles both content and thinking fields)
                response_text = extract_text_from_response(api_result)
                done_reason = api_result.get("done_reason", "?")

                if not response_text:
                    failed += 1
                    # Dump the raw API response keys for debugging
                    msg_keys = list(api_result.get("message", {}).keys())
                    emit("log", message=f"âŒ AIè¿”å›ç©ºå†…å®¹\n   å“åº”å­—æ®µ: {msg_keys}\n   done_reason: {done_reason}")
                    emit("progress", step=i + 1, total=total,
                         desc=f"å·²ç”Ÿæˆ {success_count} æ¡ï¼ˆ{failed} å¤±è´¥ï¼‰")
                    continue

                # Show AI response
                resp_display = response_text[:300].replace("\n", " ")
                emit("log", message=f"ğŸ¤– AIè¿”å›({len(response_text)}å­—): {resp_display}")

                # Parse JSON
                data = parse_json_response(response_text, mode=args.mode)
                if data:
                    # Quality check for style mode: reject if output is too similar to input
                    if args.mode == "style":
                        output_text = data.get("output", "")
                        sim = text_similarity(output_text, text)
                        if sim > 0.6:
                            failed += 1
                            similarity_rejected += 1
                            emit("log", message=f"âš ï¸ é£æ ¼æ¨¡å¼è´¨é‡æ£€æµ‹ï¼šoutputä¸åŸæ–‡ç›¸ä¼¼åº¦è¿‡é«˜({sim:.0%})ï¼Œå·²è·³è¿‡")
                            emit("progress", step=i + 1, total=total,
                                 desc=f"å·²ç”Ÿæˆ {success_count} æ¡ï¼ˆ{failed} å¤±è´¥ï¼Œ{similarity_rejected} ç›¸ä¼¼åº¦è¿‡é«˜ï¼‰")
                            continue

                    chat_data = to_chat_format(data, args.mode)
                    if chat_data:
                        success_count += 1
                        # Incremental write
                        train_file.write(json.dumps(chat_data, ensure_ascii=False) + "\n")
                        train_file.flush()
                        emit("log", message=f"âœ… æˆåŠŸ! å·²ç´¯è®¡ {success_count} æ¡\n   Q: {str(list(data.values())[0])[:60]}...")
                    else:
                        failed += 1
                        emit("log", message=f"âš ï¸ JSONå­—æ®µä¸åŒ¹é…: {list(data.keys())}")
                else:
                    failed += 1
                    emit("log", message=f"âŒ JSONè§£æå¤±è´¥\n   AIåŸæ–‡: {response_text[:400]}")

            except urllib.error.URLError as e:
                failed += 1
                emit("log", message=f"âŒ ç½‘ç»œé”™è¯¯: {e}")
            except Exception as e:
                failed += 1
                emit("log", message=f"âŒ å¼‚å¸¸: {type(e).__name__}: {e}")

            emit("progress", step=i + 1, total=total,
                 desc=f"å·²ç”Ÿæˆ {success_count} æ¡ï¼ˆ{failed} å¤±è´¥ï¼‰")

    finally:
        train_file.close()

    emit("log", message=f"\nâ•â• ç”Ÿæˆå®Œæ¯• â•â•\n   âœ… æˆåŠŸ: {success_count}\n   âŒ å¤±è´¥: {failed}\n   ğŸ“Š æ€»è®¡: {total}")

    if success_count == 0:
        emit("error", message=f"æœªç”Ÿæˆæœ‰æ•ˆæ•°æ®ï¼ˆ{total}æ®µå…¨éƒ¨å¤±è´¥ï¼‰ã€‚è¯·æŸ¥çœ‹AIæ—¥å¿—æ’æŸ¥åŸå› ã€‚")
        sys.exit(1)

    # Write valid.jsonl from the last 10% of train data
    all_results = []
    with open(train_path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                all_results.append(line.strip())

    if len(all_results) > 1:
        split_idx = max(1, int(len(all_results) * 0.9))
        valid_lines = all_results[split_idx:]
        train_lines = all_results[:split_idx]

        with open(train_path, "w", encoding="utf-8") as f:
            for line in train_lines:
                f.write(line + "\n")
        with open(valid_path, "w", encoding="utf-8") as f:
            for line in valid_lines:
                f.write(line + "\n")

        emit("log", message=f"ğŸ’¾ å·²ä¿å­˜: train.jsonl ({len(train_lines)}æ¡), valid.jsonl ({len(valid_lines)}æ¡)")
    else:
        # Only one result, copy to both
        with open(valid_path, "w", encoding="utf-8") as f:
            for line in all_results:
                f.write(line + "\n")
        emit("log", message=f"ğŸ’¾ å·²ä¿å­˜: train.jsonl ({len(all_results)}æ¡), valid.jsonl ({len(all_results)}æ¡)")

    emit("complete",
         train_count=success_count,
         failed=failed,
         total=total)


if __name__ == "__main__":
    main()
